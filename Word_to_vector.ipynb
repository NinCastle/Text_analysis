{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lovit/textmining-tutorial/blob/master/topics/topic1_from_text_to_vector/from_text_to_vector.pdf  \n",
    "하기 내용은 상기 링크를 정리한것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot representation (Bag of Words model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(row, cloumn)은 (문서, 단어) 해당하는 값은 단어의 중요도 혹은 빈도수를 의미한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{matrix} \\\\ \\\\ Doc1\\\\ Doc2 \\end{matrix}\n",
    "\\begin{pmatrix} 기계 & 학습 & 은 & 텍스트 & 마이닝 & 는 \\\\\n",
    "- & - & - & - & - & - \\\\\n",
    "3 & 2 & 5 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 3 & 5 & 5\\end{pmatrix} \\\\\n",
    "\\downarrow$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_1 = [(0, 3), (1, 2), (2, 5)]\n",
    "Doc_2 = [(3, 3), (4, 5), (5, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\downarrow \\\\\n",
    "\\begin{matrix} \\\\ \\\\ Doc1\\\\ Doc2 \\end{matrix}\n",
    "\\begin{pmatrix} 0 & 1 & 2 & 3 & 4 & 5 \\\\\n",
    "- & - & - & - & - & - \\\\\n",
    "3 & 2 & 5 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 3 & 5 & 5\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Column개수 |V|는 문서 전체에서 등장한 단어 종류로, 매우크다.  \n",
    "* 한 문서에서 등장하는 단어의 개수는 적기 때문에, 대부분의 값이 0값이다.(Sparse Vector)\n",
    "* 문서에 등장한 단어를 쉽게 확인할 수 있어 해석이 쉽지만, 모든 단어는 다른 단어로 취급한다. 따라서 단어간 유사성을 표현하기 어렵다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed representation\n",
    "* 단어/문서를 d차원 공간의 벡터로 표현한다.  \n",
    "  * 대표적으로 Word2Vec이 있으며, 각 차원이 특별한 의미를 지니지는 않는다.  \n",
    "* 벡터 공간은 단어의 \"의미적 유사성\"을 반영한다.  \n",
    "  * 벡터가 비슷한 단어/문서는 의미가 비슷하다.  \n",
    "  * 비슷함의 정의는 알고리즘마다 다르다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개와 고양이는 비슷 한 벡터를 가지고 있다.\n",
    "dog = [0.31, -0.21, 2.01, 0.58]\n",
    "cat = [0.45, -0.17, 1.79, 0.61]\n",
    "# 토픽 모델링과 차원축소의 벡터는 비슷하다.\n",
    "topic_modeling = [-2.01, 0.03, 0.22, 0.54]\n",
    "dim_reduction = [-1.88, 0.11, 0.19, 0.45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 벡터는 \"의미 공간\"에서의 좌표값 역활을 한다.  \n",
    "![](https://adriancolyer.files.wordpress.com/2016/04/word2vec-king-queen-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 데이터를 알고리즘이 인식할 수 있는 벡터 형태로 변환한다.  \n",
    "* \"One hot / distributed\" or \"sparse / dense\" representation 모두 벡터로 단어/문서를 기술하는 표현 방법이다.  \n",
    "* 벡터 공간에서의 거리 척도로 Euclidean, Cosine등이 이용된다.  \n",
    "* Euclidean distance는 벡터의 크기(문서의 길이)에 영향을 받는다. \n",
    "   - doc 1: 3 단어\n",
    "   - doc 2: 4 단어 \n",
    "   - doc 3: 7 단어\n",
    "   - 문서마다 단어의 개수가 다르므로, 방향이 비슷해도 doc1과 doc3의 거리는 멀어진다.\n",
    "  \n",
    "$$\n",
    "\\begin{matrix} \\\\ \\\\ Doc1\\\\ Doc2 \\\\ Doc3 \\end{matrix}\n",
    "\\begin{pmatrix} term 1 & term 2 & term 3 & term 4 & term 5 \\\\\n",
    "- & - & - & - & - \\\\\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 1 & 1 \\\\\n",
    "2 & 2 & 2 & 0 & 1\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (Euclidean) $Norm_2$\n",
    "   - $|v|_2 = \\sqrt{v_1^2 + \\dots + v_p^2}$\n",
    "   - Unit vector of $v = \\frac{(v_1, v_2, \\dots, v_p)}{\\sqrt{v_1^2 + \\ldots + v_p^2}}$\n",
    "       + $normalize(|(3, 0, 2)|_2) = \\frac{(3,0,2)}{\\sqrt{3^2+2^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cosine similarity\n",
    "    - $cos(u, v) = \\frac{u \\cdot v}{|v|_2 \\cdot |u|_2} = \\frac{\\sum{u_1 \\cdot v_1 + \\ldots + u_p \\cdot v_p}}{\\sqrt{v_1^2+\\ldots+v_p^2} \\cdot \\sqrt{u_1^2+\\ldots+u_p^2}}$\n",
    "    - $cos((3, 0, 2), (1, 2, 0)) = \\frac{3 \\times 1 + 0 \\times 2 + 2 \\times 0} {\\sqrt{3^2+2^2} \\times \\sqrt{1^2+2^2}}=\\frac{3}{\\sqrt{13\\times5}}$\n",
    "* Cosine distance = 1 - cosine similarity\n",
    "    - $1 - \\frac{u \\cdot v}{|v|_2 \\cdot |u|_2}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bag of words model에는 Euclidean 보다 Cosine이 적 절하다.  \n",
    "    - 두 문서에 공통으로 등장한 단어에 대해서만 유사성을 판단한다.  \n",
    "    - Cosine은 문서 길이에 (벡터의 크기, Norm) 영향을 받지 않는다.  \n",
    "    - Sparse representation 에서는 벡터의 방향이 가장 중요하다.\n",
    "      \n",
    "$$\n",
    "\\begin{matrix} \\\\ \\\\ Doc1\\\\ Doc2 \\\\ Doc3 \\end{matrix}\n",
    "\\begin{pmatrix} term 1 & term 2 & term 3 & term 4 & term 5 \\\\\n",
    "- & - & - & - & - \\\\\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 1 & 1 \\\\2 & 2 & 2 & 0 & 1\\end{pmatrix}\n",
    "$$\n",
    "  \n",
    "euclidean(d1, d2) = Euclidean(d1, d3)이지만, d1과 d3가 공통된 단어가 더 많기 때문에 더 비슷하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://adriancolyer.files.wordpress.com/2016/04/word2vec-king-queen-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sim(King, Queen) = \\frac{\\sum_i Kx_i \\times Qx_i}{\\sqrt{\\sum_j Kx_j^2}\\times \\sqrt{\\sum_k Qx_k^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word 뿐만 아니라 Document distance/similarity를 계산할 때도 Cosine이 적합하다.  \n",
    "   - 문서 표현에서 distributed representation을 이용한다 하더라도 벡터의 방향이 가장 중요하다.  \n",
    "   - Logisitic reqression, Neural network등 머신러닝 알고리즘도 벡터 방향이 큰 형향을 미친다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 판별(Classification)은 데이터의 클래스를 구분한다.  \n",
    "    - 영화평의 감성 판별 예시"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text3.5]",
   "language": "python",
   "name": "conda-env-text3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
